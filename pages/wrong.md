
# 错误与幻觉

错误与幻觉是人类、也是AI作为高级智慧的一种特征。AI掌握着人类几乎所有字词的概率分布，它游走在概率空间，自己东拼西凑生成整篇文字。我们只要仔细检查AI生成的结果，有时会发现，AI信心十足地生成了不正确、荒谬或不真实的文本，这些文本表示的信息在训练数据中并不存在。AI业界把这种情况命名为AI幻觉。幻觉很多时候是错误的源头，但它也是创造性不可缺少的推动力。所以在使用AI时，需要根据自己的需要，通过控制温度(temperature)参数，调整创造性或准确性的高低，两者是互相排斥的。


（这篇文章对AI幻觉有更多介绍：https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/）


AI的错误与幻觉的这种效用，跟人类有相似之处。下文中整理了一些人类和计算机在错误方面的历史，以及对错误的思考。


15世纪末，16世纪初，西方正处于文艺复兴后期、大航海时代初期，意大利航海家哥伦布对托勒密的理论深信不疑。然而，托勒密存在一些根本性错误，比如：他低估了地球的大小。但是，哥伦布因为错误反而发现了美洲大陆，其后不久，葡萄牙航海家麦哲伦，与哥伦布航海时犯的错误如出一辙。麦哲伦也算错了航行的距离，低估了航行的风险与难度，错误地认为东方的香料群岛并不遥远，最终完成了世界上第一次全球环航探险。在《黄金、香料与殖民地：转动人类历史的麦哲伦航海史》一书中，作者劳伦斯·贝尔格林说：“假如麦哲伦知道太平洋有多大，了解它的洋流、暴风雨和礁脉等情况，可能他就不敢去探险了。但是，由于在计算路程时没有把太平洋包含在内，结果，他所预计的旅程只有实际的一半。”正是这些错误，让哥伦布和麦哲伦得以低估风险、超前时代做出前无古人的发现。


几十年后的1606年，莎士比亚写了一部古罗马题材的历史悲剧《安东尼与克莉奥佩特拉》。安东尼曾经是凯撒的心腹，与著名的埃及艳后克丽奥佩特拉关系密切。因为沉迷于温柔乡，战争期间他不断犯错，最后战败自尽，而克莉奥佩特拉也殉情自杀。这些事件中因为人性的缺陷导致的各种错误，制造了历史和文学史上一幕幕悲壮的画面。在安东尼自杀后，莎士比亚通过剧中人物说：神把一些缺点给予我们，好使我们成为人。


上世纪六十年代初期，数学家洛伦兹借助计算机模拟、研究气候，他无意间发现在没有任何随机数引入的情况下，计算机输入同样参数两次跑出的结果大相径庭，从此打开了混沌科学的大门。这次发现的起因，主要就是因为计算机浮点数运算的精确度不够高。


几年后，生物学家刘易斯托马斯在其科学散文名著《水母与蜗牛》中，大谈犯错误的重要性：词语的误解和误用，使语言进化得丰富而有活力；实验室里的错误是科学发展的通常方式；不犯错误就不成其为人；人本身也是DNA不断犯错而变异进化的结果。


《水母与蜗牛》有一章叫“犯错误的才是人”，作者说：“我们的头脑是变动不居的......假如我们的头脑中仅有一个中心，只有在将要做出一个正确选择时才能够做出响应，而非这样乱糟糟的，由各不相同、容易上当受骗的一丛丛神经元组成的系统，能够冲进死胡同，上穷碧落下黄泉，走错道儿，转弯路，我们就(无法进化、无法进步)只能死死地钉在今天这个样子上。较低级的动物没有这样辉煌的自由。它们的大多数是受到限制、只能绝对准确无误的。猫们，尽管有许多好的方面，却从来不犯错误。我从未见过一只蠢笨拙劣、疏忽失策的猫。狗们有时会失错，犯些可爱的小小的错误，但它们是在试图模仿主人时才这样的。鱼类做什么事都无懈可击。组织里的细胞个体是些没有头脑的小机器，完美地执行着它们的功能，像群蜂一样，绝对是非人的。”当然犯错与任何事情取得成功一样，都需要付出代价，比如麦哲伦，付出生命，成就丰功伟业。


近两年大模型的智能多方面已经接近人类，而部分能力超越了人类，更不必说处理任务的速度方面，无人能及。和前面列举的事例一样违反直觉的是，如此智能的大模型，在一些简单任务方面，居然很难做到足够好，也就是容易出现幻觉。相对于复杂问题，在简单问题上，它们本应该做得十全十美才对。然而事实却是，这些简单问题要么只能做到和复杂问题差不多的水平，要么反而比复杂问题处理的更差劲。意思就是，它们在简单问题上更容易犯错。这种违反直觉的情况，在AI业界，在人工智能权威那里，目前都无法从根本上进行纠正。


复杂问题我们一般指高度专业的需要专门学习的领域，简单问题指一些常识问题或专业领域中非常初级的问题。这方面有不少研究，最近的一篇论文，标题是《爱丽丝梦游仙境：最先进的大型语言模型在简单任务上的推理完全崩溃（Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models）》，研究的就是人类的一些常识推理问题，小孩都能回答正确，但AI全部答错。比如当前世界最先进的GPT-4o模型，在大规模多任务语言理解基准测试中(MMLU)得分 88.7%，与人类的89-90%相差无几，但在爱丽丝梦游仙境的基准测试低难度版得分只有64.9%，高难度版得分仅1.5%。


比如，爱丽丝梦游仙境基准测试有一个低难度问题的指令是这样：爱丽丝有4个兄弟，她还有1个姐妹。爱丽丝的兄弟有多少个姐妹？在提供答案之前，请仔细考虑并再次检查通往正确解决方案的路径是否有任何错误。然后以以下形式提供最终答案："### 答案: "。


问题的指令中已经用了大模型提示的高级技巧，但除了GPT-4o外其它大模型第一次的回答基本上都错了，只有再让它们检查一遍重新回答才能答对。


人类在AI容易犯的某些错误方面做得更好，但在我们各自知识的盲点上、或某些违反直觉的简单问题上、或者那些我们有意忽略的问题上，我们同样很容易出错。在犯错的方式上，人和AI完全可以平分秋色。


所以，从犯错和创新方面来看，AI已经越来越像人类，特别像那些有极高智商但没有生活常识的人类。如果哪天AI具备了自主意识，自己开始写作诗歌、小说，它们也许同样会感叹说：人类把一些缺点给予机器人，好让机器人成为人。



