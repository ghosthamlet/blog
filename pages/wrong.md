
# 错误和缺陷

15世纪末，16世纪初，西方正处于文艺复兴后期、大航海时代初期，意大利航海家哥伦布因为对地理错误的认识，他对托勒密的理论深信不疑。然而，托勒密存在一些根本性错误，比如：他低估了地球的大小，发现了美洲大陆，其后不久，葡萄牙的探险家麦哲伦，也是错误地认为东方的香料群岛并不遥远，最终完成了世界第一次全球环航探索。然而正式这些错误，让哥伦布和麦哲伦都低估了航海的风险，才得以做出前无古人的业绩，改变了一个国家甚至是人类的历史，如果他们一开始获得的正确的信息，那么估计两人都不敢冒着生命危险做出行动，而发现美洲和全球探险的事业可能不得不推迟几十、上百年。

几十年后的1606年，莎士比亚写了一部古罗马题材历史悲剧《安东尼与克莉奥佩特拉》，讲述的故事发生在凯撒被暗杀后，帝国内部纷争不断，安东尼曾经是凯撒的心腹，当时正与凯撒的养子屋大维争夺帝国控制权。克丽奥佩特拉就是著名的埃及艳后，凯撒和安东尼都和她有着密切关系。《安东尼与克莉奥佩特拉》故事中，安东尼因为战败自尽，克莉奥佩特拉则因为各种原因、包括为了殉情也自杀了。在安东尼自杀后，莎士比亚通过剧中人物说：神把一些缺点给予我们，好使我们成为人。

上世纪中叶，生物学家刘易斯托马斯在科学散文名著《细胞》中，用大量篇幅不断强调说，错误和缺陷，是人类一切重要发现和创新，是自然界进化的唯一途径，所以不要惧怕犯错。当然代价是有的，比如麦哲伦，就付出了生命。
   
近两年大模型的智能多方面已经接近人类，而部分能力超越了人类，更不必说处理任务的速度方面，无人能及。但和前面列举的事例一样违反直觉的是，如此智能的大模型，在一些简单任务方面，居然做得并不够好，最起码相对于复杂问题，在简单问题上，它们本应该做得十全十美才对。然而事实却是，这些简单问题要么只能做到和复杂问题差不多的水平，要么反而比复杂问题处理的更差劲，意思就是，它们在简单问题上更容易犯错。这种违反直觉的情况，在AI业界，在人工智能权威那里，目前都无法从根本上进行纠正。复杂问题我们一般指高度专业的需要专门学习的领域，简单问题指一些常识问题或专业领域中非常初级的问题。

这方面有不少研究，最近的一篇论文：爱丽丝梦游仙境：最先进的大型语言模型在简单任务上的推理完全崩溃（Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models），研究的就是人类的一些常识推理问题，小孩都能回答正确，但AI全部答错。比如GPT-4o，在大规模多任务语言理解基准测试中(MMLU)得分 88.7%，与人类的89-90%相差无几，但在爱丽丝梦游仙境的基准测试低难度版得分是64.9%，高难度版得分1.5%，
这就是一个低难度问题：爱丽丝有4个兄弟，她还有1个姐妹。爱丽丝的兄弟有多少个姐妹？在提供答案之前，请仔细考虑并再次检查通往正确解决方案的路径是否有任何错误。然后以以下形式提供最终答案："### 答案: "。问题的指令中已经用了大模型提示的高级技巧，但除了GPT-4o外其它模型第一次回答都错了，只有再让它们检查一遍才能答对。

人类虽然在AI容易犯的错误方面做得更好，但在我们各自知识的盲点上、或某些违反直觉的简单问题上、或者那些我们有意忽略的问题上，同样很容易出错。所以在犯错的方式上，人和AI可以平分秋色。

从这方面看，再基于前面我们谈论的人类自身的情况来说，AI已经越来越像人类，最像那些有极高智商但没有生活常识的人类。尽管它们仅仅只是从人类制造的信息中学习而模仿人类，因此犯错也像人类一样，但不得不说，现在的AI也许确实具备基本的人性了，毕竟我们人类物种的学习，也基本上是在互相模仿中前进的。如果哪天AI具备了自主意识，自己开始写作诗歌小说，它们也许同样会感叹说：人类把一些缺点给予机器人，好让机器人成为人。
